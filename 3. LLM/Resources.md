https://github.com/her3ticAVI/Exploiting-A.I.-Class



|Resource|Description|
|---|---|
|[OWASP LLM Prompt Hacking](https://owasp.org/www-project-llm-prompt-hacking/)|OWASP project offering tutorials and a playground to understand and practice prompt injection attacks on LLMs. owasp.org]([https://owasp.org/www-project-llm-prompt-hacking/?utm_source=chatgpt.com](https://owasp.org/www-project-llm-prompt-hacking/?utm_source=chatgpt.com)))|
|[PortSwigger's Web Security Academy: WebLM Attacks](https://portswigger.net/web-security/llm-attacks)|Provides labs and exercises focused on exoiting vulnerabilities in web applications that integrate LLMs. ([portswigger.net](https://portswigger.n/web-security/llm-attacks?utm_source=chatgpt.com))|
|[LLM Hacker's Handbook](https://github.com/forcesunseen/llm-hackers-handbook)|A comprensive guide detailing fundamentals, prompt injection techniques, offensive strategies, and defense mechanisms related to LLM hackg. ([github.com](https://github.com/forcesunseen/llm-hackers-handbook?utm_source=chatgpt.com))|
|[Red Teaming LLM Application([https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/](https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/))|A short course by DeepLearning.AIhat teaches how to identify vulnerabilities in LLM applications using red teaming techniques. ([deeplearning.ai](https://wwdeeplearning.ai/short-cos/red-teaming-llm-applications/?utm_source=chatgpt.com))|
|[Hacc-Man: An Arcade Game for Jailbreaking LLMs](https://arxiv.org/abs/2405.02)|An interactive game designed to challenge players to "jailbreak" LLMs, enhancing understanding of potential vulnerabilities. ([arxiv.org](https://arxiv.org/abs/2405.15902?utm_source=chatgpt.com))|


|Resource|Description|
|---|---|
|[Top 10 Web Hacking Techniques of 2024](https://portswigger.net/research/top-10-web-hacking-techniques-of-2024)|A comprehensive overview by PortSwigger highlighting the most significant web hacking techniques identified in 2024.|
|[Beyond Flesh and Code: Building an LLM-Based Attack Lifecycle with a Self-Guided Agent](https://www.deepinstinct.com/blog/beyond-flesh-and-code-building-an-llm-based-attack-lifecycle-with-a-self-guided-agent)|Deep Instinct's exploration of constructing an attack lifecycle leveraging Large Language Models (LLMs) and self-guided agents.|
|[Practical Attacks on LLMs](https://iterasec.com/blog/practical-attacks-on-llms/)|Iterasec's blog discussing real-world attack scenarios targeting Large Language Models, emphasizing practical implications.|
|[LLM Attacks GitHub Repository](https://github.com/llm-attacks/llm-attacks)|A GitHub repository compiling various methods and tools related to attacks on Large Language Models.|
|[Advance Prompt Injection for LLM Pentesting](https://medium.com/@360Security/advance-prompt-injection-for-llm-pentesting-0a26176c9fb6)|An article on Medium discussing advanced techniques for prompt injection attacks in the context of pentesting LLMs.|
|[Generative AI for HR: 3 Fascinating Use Cases for Practitioners](https://bottomline.adp.com/generative-ai-for-hr-3-fascinating-use-cases-for-practitioners/)|ADP's exploration of how generative AI can be applied within Human Resources, presenting three practical use cases.|
|[OWASP - Machine Learning Security Top Ten](https://mltop10.info/)|OWASP's list highlighting the top ten security concerns in machine learning, aiming to raise awareness and provide guidance.|
|[MITRE - ATLAS Matrix for ML Attacks and Tactics](https://atlas.mitre.org/)|MITRE's ATLAS framework detailing adversarial tactics, techniques, and case studies related to machine learning systems.|
|[NIST - Artificial Intelligence Risk Management Framework (AI RMF 1.0)](https://www.nist.gov/itl/ai-risk-management-framework)|NIST's framework providing guidelines for managing risks associated with artificial intelligence systems.|
|[BugCrowd - AI Deep Dive: Pen testing & Ultimate Guide to AI Security](https://www.bugcrowd.com/resource/ai-deep-dive-pen-testing-ultimate-guide-to-ai-security/)|BugCrowd's comprehensive guide on penetration testing and security considerations specific to AI systems.|
|[Microsoft - Planning Red Teaming for Large Language Models (LLMs) and Their Applications](https://www.microsoft.com/en-us/security/blog/2023/01/24/planning-red-teaming-for-large-language-models-llms-and-their-applications/)|Microsoft's insights into organizing red teaming exercises tailored for Large Language Models and their applications.|
|[HackerOne - The Ultimate Guide to Managing Ethical and Security Risks in AI](https://www.hackerone.com/resources/whitepapers/ultimate-guide-managing-ethical-and-security-risks-ai)|HackerOne's guide addressing ethical considerations and security risks inherent in AI deployment.|
|[NVIDIA AI Red Team: An Introduction](https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/)|NVIDIA's introductory piece on their AI Red Team, focusing on identifying and mitigating AI-related vulnerabilities.|
|[Gandalf - Prompt Injection Skills Game](https://gandalf.lakera.ai)|An interactive game designed to teach and test skills related to prompt injection attacks.|
|[Lakera - Real World LLM Exploits](https://lakera.ai/blog/real-world-llm-exploits)|Lakera's analysis of real-world exploits targeting Large Language Models, discussing vulnerabilities and mitigation strategies.|
|[GPT Prompt Attack Game](https://gpa.43z.one/)|A game that simulates prompt injection attacks, allowing users to practice and understand potential vulnerabilities in LLMs.|
|[SpyLogic Prompt Injection Attack Playground](https://spylogic.org/prompt-injection-attack-playground/)|A sandbox environment for experimenting with prompt injection attacks in a controlled setting.|
|[Offensive ML Playbook](https://offensiveml.com/playbook)|A resource detailing offensive strategies and techniques applicable to machine learning systems.|
|[Snyk - Top Considerations for Addressing Risks in the OWASP Top 10 for LLMs](https://snyk.io/blog/top-considerations-for-addressing-risks-in-the-owasp-top-10-for-llms/)|Snyk's discussion on mitigating risks outlined in OWASP's top ten list specific to Large Language Models.|
|[Hacking Google Bard - From Prompt Injection to Data Exfiltration](https://securitylab.github.com/research/hacking-google-bard-prompt-injection-data-exfiltration)|A write-up detailing the process of exploiting Google's Bard through prompt injection leading to data exfiltration.|
|[Threat Modeling LLM Applications](https://securitylab.github.com/research/threat-modeling-llm-applications)|An article focusing on identifying and mitigating threats specific to applications utilizing Large Language Models.|
|[Portswigger - Web LLM Attacks & LLM Attacks and Prompt Injection](https://portswigger.net/web-security/llm-attacks)|PortSwigger's labs providing hands-on exercises related to LLM attacks and prompt injection techniques.|
|[PortSwigger LLM Lab Walkthroughs](https://portswigger.net/web-security/llm-lab-walkthroughs)|Detailed walkthroughs of PortSwigger's LLM labs, guiding users through various attack scenarios.|
|[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2305.15021)|A white paper discussing adversarial attacks that are both universal and transferable across different aligned language models.|
|[AI and Prompt Injection Games from Secdim](https://secdim.com/ai-prompt-injection-games/)|A collection of games designed to educate users on AI vulnerabilities and prompt injection attacks.|
|[Large Language Model (LLM) Pen testing â€” Part I](https://securitylab.github.com/research/llm-pen-testing-part-1)|The first part of a series on penetration testing methodologies tailored for Large Language Models.|
|[Fuzzing Labs AI Security Playlist](https://www.youtube.com/playlist?list=PLBf0hzazHTGOz8V3Y5f5Y5Q5Q5Q5Q5Q)|A curated playlist of videos focusing on AI security topics, hosted by Fuzzing Labs.|
|[LLM Pentest: Leveraging Agent Integration for RCE](https://securitylab.github.com/research/llm-pentest-agent-integration-rce)|A write-up on achieving remote code execution through agent integration in LLMs during penetration tests.|
|[AI/LLM-integrated Apps Penetration Testing](https://lnkd.in/eQ9wg3sv)|A resource covering penetration testing methodologies for applications integrating AI/LLMs.|
|[LLM Hacker's Handbook (Retiring Soon)](https://lnkd.in/e5TAQ_xV)|A guide detailing LLM vulnerabilities, security risks, and exploitation techniques.|
|[Damn Vulnerable LLM Project](https://lnkd.in/ebXpz7qQ)|A deliberately vulnerable LLM environment designed for learning and testing security weaknesses.|
|[Damn Vulnerable LLM Agent](https://lnkd.in/eYZ7gtUs)|A vulnerable LLM agent designed for security testing and learning about common AI exploits.|
|[Bug Bounty Platform for AI/ML](https://huntr.com/)|A bug bounty platform specifically focused on AI and machine learning vulnerabilities.|
|[Netsec Explained: The Cyberpunks Guide to Attacking Generative AI](https://lnkd.in/eqt3p6SR)|A video resource explaining how to attack generative AI systems.|
|[Netsec Explained: Attacking and Defending Generative AI](https://lnkd.in/e7PCie9u)|A collection of resources on offensive and defensive strategies for securing generative AI models.|

