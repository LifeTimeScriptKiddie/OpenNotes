# 1.  Simple System Flow

BLUF: Finalized Script on Venv is here. 
[1. main_streamlit.py](<Venv/1. main_streamlit.py.md>)
[2. requirements.txt](<Venv/2. requirements.txt.md>)s
[3. start_tmux.sh](<Venv/3. start_tmux.sh.md>)




```mermaid
flowchart TD
    A[ðŸ“¥ Input: Raw PDFs in /pdfs] --> B{"Check: Is PDF text-based? (pdffonts)"}
    
    B -- Yes --> C[ðŸ§ª Try extracting text with pdffonts + PyPDF2]
    C --> D{Is the extracted text sufficient?}
    
    D -- Yes --> E[âœ… Save extracted text to /extracted_text]
    D -- No --> F[ðŸ” Run OCRmyPDF on original PDF]
    
    B -- No --> F
    F --> G[ðŸ“„ Output OCR'd PDF to /ocr_output_pdfs]
    G --> H[ðŸ§  Extract text from OCR'd PDF]
    H --> E
    
    E --> I{More PDFs to process?}
    I -- Yes --> B
    I -- No --> J[ðŸ Done: All PDFs processed and text extracted]

```

---
## 1. PDF Input

> **Purpose:**Â Supply raw, unstructured data in a format accessible to humans.

- **Why?**Â Most real-world knowledge is locked in PDFs â€” policies, reports, academic work, manuals.
    
- **System View:**Â Static file input; varies wildly in structure and size.
    
- **Failure Mode:**Â PDFs can be encrypted, malformed, or image-only (which complicates downstream processing).
    

---

## 2. Text Extraction

> **Purpose:**Â Convert human-readable documents into machine-readable text.

- **Why?**Â LLMs can't â€œseeâ€ what's inside PDFs â€” they need tokenized strings.
    
- **Tools:**
    
    - For text PDFs:Â `pdfplumber`,Â `PyMuPDF`,Â `langchain`Â loaders.
        
    - For image PDFs:Â `pdf2image`Â â†’Â `pytesseract`Â (OCR).
        
- **Failure Mode:**Â OCR might misread poor-quality images; wrong encoding (e.g., ligatures) might distort content.
    

---

## 3. Text Chunking

> **Purpose:**Â Break large documents into logical, overlapping pieces for LLM input.

- **Why?**Â LLMs haveÂ **limited context windows**Â (e.g., 4Kâ€“8K tokens). Full documents wonâ€™t fit.
    
- **Chunking logic:**
    
    - Maintain semantic structure (avoid mid-sentence breaks).
        
    - Add overlap (e.g., 10â€“20%) to preserve context across chunks.
        
- **Failure Mode:**Â Bad chunking = broken context = reduced retrieval quality.
    

---

## 4. Embedding

> **Purpose:**Â Convert text chunks into vectors representingÂ **semantic meaning**.

- **Why?**Â Enables searching byÂ **meaning**, not exact keywords.
    
- **Example:**
    
    - â€œEmployees must attend training.â€ â‰ˆ â€œAnnual training is mandatory.â€
        
    - Both will generateÂ **similar embeddings**, enabling powerful recall.
        
- **Failure Mode:**Â Wrong embedding model = bad semantic matches.
    

---

## 5. Vector Database

> **Purpose:**Â Store and index high-dimensional vectors efficiently.

- **Why?**Â With 5,000+ documents, you need sub-second search based on vector proximity (cosine similarity, etc).
    
- **Tools:**Â `FAISS`,Â `Chroma`,Â `Qdrant`,Â `Weaviate`
    
- **Failure Mode:**Â Large vector sets may grow out of RAM; poorly indexed DBs will be slow.
    

---

## 6. Similarity Search (Retriever)

> **Purpose:**Â Match a query to the most relevant content chunks.

- **Why?**Â LLMs are smart â€” but onlyÂ **within the context you give them**.
    
- **Process:**
    
    - Query â†’ embedding â†’ search â†’ top-k relevant chunks.
        
- **Failure Mode:**Â Bad retrieval = hallucination. Garbage in, garbage out.
    

---

## 7. Prompt Construction

> **Purpose:**Â Assemble retrieved content + user question into a prompt the LLM can reason over.

- **Why?**Â LLMs areÂ **stateless**Â and context-blind. You must feed them relevant information at runtime.
    
- **Example Prompt:**
    
    ```text
    Context:
    [Chunk 1]
    [Chunk 2]
    
    Question: What should employees do after a breach?
    
    Answer:
    ```
    
- **Failure Mode:**Â Unstructured prompts = poor reasoning = inconsistent output.
    

---

## 8. LLM Response

> **Purpose:**Â Generate human-readable, natural-language answers.

- **Why?**Â This is the final output â€” theÂ **"consultant's answer"**Â based on retrieved knowledge + language modeling.
    
- **Tools:**Â `llama.cpp`,Â `vllm`,Â `text-generation-webui`,Â `Ollama`, etc.
    
- **Failure Mode:**Â If context is weak or too generic, LLM will fabricate or be vague.
    



---
Next Process

[1. Text Extraction](<1. Text Extraction.md>)