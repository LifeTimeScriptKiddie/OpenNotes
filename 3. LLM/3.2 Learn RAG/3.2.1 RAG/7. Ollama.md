This script will do, 
âœ… **Generating a Streamlit app** that integrates **RAG retrieval** with a **local LLM (Ollama)** via subprocess.
âœ… Saving the app to /mnt/data/rag_streamlit_with_ollama/app.py.
âœ… Zipping the app into /mnt/data/rag_streamlit_with_ollama.zip for easy transfer/deployment.

---

### **To Run Everything:**

#### **1. Install requirements:**

```
pip install streamlit sentence-transformers chromadb
```
#### **2. Start the app:**

```
streamlit run /mnt/data/rag_streamlit_with_ollama/app.py
```

#### **3. Ensure Ollama is running:**

Make sure a model (e.g., llama3) is downloaded and Ollama is installed and serving:

```
ollama run llama3
```

---

### **ðŸ“¦ Output:**

Youâ€™ll get:

- A **web app UI** with input for user question    
- Embeds the query using MiniLM
- Fetches chunks from ChromaDB
- Builds prompt
- Sends prompt to ollama run llama3
- Displays result in browser
    

```python
# Generate Streamlit app with direct integration to Ollama (via subprocess call)
from pathlib import Path
import zipfile

# Define the app path
app_path = Path("/mnt/data/rag_streamlit_with_ollama")
app_path.mkdir(parents=True, exist_ok=True)

# Updated Streamlit app with Ollama integration
streamlit_ollama_code = """
import streamlit as st
from sentence_transformers import SentenceTransformer
import chromadb
import subprocess

st.set_page_config(page_title="RAG with Ollama", layout="wide")
st.title("ðŸ§  RAG + Ollama: Ask Questions from Your PDFs")

query = st.text_input("Ask a question from your documents:")

if query:
    model = SentenceTransformer('all-MiniLM-L6-v2')
    chroma_client = chromadb.Client()
    collection = chroma_client.get_collection("rag_chunks")

    # Embed query
    query_vector = model.encode(query).tolist()
    results = collection.query(query_embeddings=[query_vector], n_results=3)
    contexts = results['documents'][0]

    st.subheader("ðŸ“„ Retrieved Contexts")
    for i, chunk in enumerate(contexts):
        st.markdown(f"**Chunk {i+1}:**")
        st.code(chunk, language="markdown")

    # Construct prompt
    context_str = "\\n---\\n".join(contexts)
    full_prompt = f\"\"\"Context:
{context_str}

Question: {query}
Answer:\"\"\"

    st.subheader("ðŸ§  Prompt Sent to LLM")
    st.code(full_prompt, language="markdown")

    # Run with Ollama
    st.subheader("ðŸ¤– Response from Ollama")
    try:
        result = subprocess.run(
            ["ollama", "run", "llama3"],
            input=full_prompt,
            capture_output=True,
            text=True,
            check=True
        )
        st.success(result.stdout.strip())
    except subprocess.CalledProcessError as e:
        st.error("âŒ Error running Ollama.")
        st.text(e.stderr)
else:
    st.info("Enter a question to get started.")
"""

# Save the app file
streamlit_ollama_path = app_path / "app.py"
streamlit_ollama_path.write_text(streamlit_ollama_code.strip())

# Zip it
zip_path = "/mnt/data/rag_streamlit_with_ollama.zip"
with zipfile.ZipFile(zip_path, 'w') as zipf:
    zipf.write(streamlit_ollama_path, "rag_streamlit_with_ollama/app.py")

zip_path

```


After Venv

```

pip install streamlit sentence-transformers chromadb
streamlit run app.py
ollama run llama3

```

Folder Structure
```
project/
â”œâ”€â”€ main.py                   # Your PDF -> ChromaDB pipeline
â”œâ”€â”€ /pdfs/                    # Drop PDFs here
â”œâ”€â”€ /extracted_text/          # Auto-generated .txt files
â”œâ”€â”€ /chunks/                  # Chunked JSON
â”œâ”€â”€ /ocr_output_pdfs/         # OCR'd PDF copies
â”œâ”€â”€ /venv/                    # Optional: Python virtualenv
â””â”€â”€ /rag_streamlit_with_ollama/
    â””â”€â”€ app.py                # Streamlit UI
```
