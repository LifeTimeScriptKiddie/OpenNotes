  

Youâ€™ll get a **single Docker container** that:

1. Runs the Ollama server with a model like llama3
    
2. Includes all your RAG pipeline code
    
3. Can process PDFs, embed, store in ChromaDB, and generate answers via LLM
    

---

## **ğŸ³ Step-by-Step: Dockerized RAG + Ollama Setup**

---

### **# 1. Directory Structure**

```
rag-ollama/
â”œâ”€â”€ app.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ supervisord.confÂ 
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ chunks/
â”‚   â”œâ”€â”€ extracted_text/
â”‚   â”œâ”€â”€ ocr_output_pdfs/
â”‚   â””â”€â”€ pdfs/
```



---

### **# 2.**Â **Dockerfile**

```
FROM ollama/ollama:latest

  

# Install Python and dependencies

RUN apt-get update && apt-get install -y --no-install-recommends \

python3 python3-pip git tesseract-ocr poppler-utils && \

rm -rf /var/lib/apt/lists/*

WORKDIR /app

  

# Copy app code

COPY app.py /app/

COPY requirements.txt /app/

COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

COPY ./app /app/app

  

# Install Python dependencies

RUN pip install --no-cache-dir -r /app/requirements.txt \

streamlit sentence-transformers chromadb

# Expose Ollama and Streamlit ports

EXPOSE 11434 8501

  

# Start Ollama and Streamlit app

# For debugging purposes, you can run the following command:

# CMD ["bash", "-c", "ollama serve & sleep 5 && streamlit run ./app.py --server.port=8501 --server.address=0.0.0.0"]

  

RUN apt-get install -y supervisor

COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

CMD ["/usr/bin/supervisord"]

# Note: The CMD line above uses supervisord to manage both Ollama and Streamlit processes.

# If you want to run them separately, you can use the commented CMD line above.

# Note: Make sure to have the supervisord.conf file in the same directory as this Dockerfile.

# The supervisord.conf file should contain the necessary configuration to run both Ollama and Streamlit.

# Example supervisord.conf content:

# [supervisord]

# nodaemon=true

# [program:ollama]

# command=ollama serve
```

---

### 3. requirements.txt

```
PyPDF2
pytesseract
pdf2image
Pillow
tqdm
ocrmypdf
langchain==0.1.16
sentence-transformers
chromadb
```


### All in one


---

### 4. main.py

[[../6. main.py]]


---

### **# 5. Build and Run**

```bash
chmod +x init.sh
./init.sh
cd rag-ollama
docker builder prune -f
docker build -t rag-ollama .
docker run -it --rm -p 8501:8501 -v "$(pwd)/app/pdfs:/app/app/pdfs" rag-ollama
```

âœ… This will process your PDFs, embed them, start the Ollama server, and run the full RAG flow.

---

### **ğŸ§ª Optional Enhancements**

|**Feature**|**Add?**|
|---|---|
|ğŸ–¼ï¸ Streamlit GUI|Yes|
|ğŸŒ Ollama API server (REST calls)|Yes|
|ğŸ’¾ Persistent ChromaDB volume|Yes|
|ğŸ”„ Auto-refresh on new PDFs|Yes|

---
