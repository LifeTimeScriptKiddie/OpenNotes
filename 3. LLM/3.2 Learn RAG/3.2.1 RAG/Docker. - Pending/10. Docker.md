  

You’ll get a **single Docker container** that:

1. Runs the Ollama server with a model like llama3
    
2. Includes all your RAG pipeline code
    
3. Can process PDFs, embed, store in ChromaDB, and generate answers via LLM
    

---

## **🐳 Step-by-Step: Dockerized RAG + Ollama Setup**

---

### **# 1. Directory Structure**

```
rag-ollama/
├── app.py
├── Dockerfile
├── README.md
├── requirements.txt
├── supervisord.conf 
├── app/
│   ├── main.py
│   ├── chunks/
│   ├── extracted_text/
│   ├── ocr_output_pdfs/
│   └── pdfs/
```



---

### **# 2.** **Dockerfile**

```
FROM ollama/ollama:latest

  

# Install Python and dependencies

RUN apt-get update && apt-get install -y --no-install-recommends \

python3 python3-pip git tesseract-ocr poppler-utils && \

rm -rf /var/lib/apt/lists/*

WORKDIR /app

  

# Copy app code

COPY app.py /app/

COPY requirements.txt /app/

COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

COPY ./app /app/app

  

# Install Python dependencies

RUN pip install --no-cache-dir -r /app/requirements.txt \

streamlit sentence-transformers chromadb

# Expose Ollama and Streamlit ports

EXPOSE 11434 8501

  

# Start Ollama and Streamlit app

# For debugging purposes, you can run the following command:

# CMD ["bash", "-c", "ollama serve & sleep 5 && streamlit run ./app.py --server.port=8501 --server.address=0.0.0.0"]

  

RUN apt-get install -y supervisor

COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

CMD ["/usr/bin/supervisord"]

# Note: The CMD line above uses supervisord to manage both Ollama and Streamlit processes.

# If you want to run them separately, you can use the commented CMD line above.

# Note: Make sure to have the supervisord.conf file in the same directory as this Dockerfile.

# The supervisord.conf file should contain the necessary configuration to run both Ollama and Streamlit.

# Example supervisord.conf content:

# [supervisord]

# nodaemon=true

# [program:ollama]

# command=ollama serve
```

---

### 3. requirements.txt

```
PyPDF2
pytesseract
pdf2image
Pillow
tqdm
ocrmypdf
langchain==0.1.16
sentence-transformers
chromadb
```


### All in one


---

### 4. main.py

[[../6. main.py]]


---

### **# 5. Build and Run**

```bash
chmod +x init.sh
./init.sh
cd rag-ollama
docker builder prune -f
docker build -t rag-ollama .
docker run -it --rm -p 8501:8501 -v "$(pwd)/app/pdfs:/app/app/pdfs" rag-ollama
```

✅ This will process your PDFs, embed them, start the Ollama server, and run the full RAG flow.

---

### **🧪 Optional Enhancements**

|**Feature**|**Add?**|
|---|---|
|🖼️ Streamlit GUI|Yes|
|🌍 Ollama API server (REST calls)|Yes|
|💾 Persistent ChromaDB volume|Yes|
|🔄 Auto-refresh on new PDFs|Yes|

---
