```bash
#!/bin/bash

  

ROOT="rag-ollama"

  

echo "ðŸ”§ Creating project directory structure..."

mkdir -p "$ROOT/app/chunks"

mkdir -p "$ROOT/app/ocr_output_pdfs"

mkdir -p "$ROOT/app/extracted_text"

mkdir -p "$ROOT/app/pdfs"

  

echo "ðŸ“ Writing Dockerfile..."

cat > "$ROOT/Dockerfile" <<EOF

FROM ollama/ollama:latest

  

# Install Python and dependencies

RUN apt-get update && apt-get install -y --no-install-recommends \\

python3 python3-pip git tesseract-ocr poppler-utils supervisor && \\

rm -rf /var/lib/apt/lists/*

  

WORKDIR /app

  

# Copy app code

COPY app.py /app/

COPY ./app /app/app

COPY requirements.txt /app/

COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

  

# Install Python dependencies

RUN pip install --no-cache-dir -r /app/requirements.txt \\

streamlit sentence-transformers chromadb

  

# Expose Ollama and Streamlit ports

EXPOSE 11434 8501

  

# Start Supervisor

CMD ["/usr/bin/supervisord"]

EOF

  

echo "ðŸ“¦ Writing requirements.txt..."

cat > "$ROOT/requirements.txt" <<EOF

PyPDF2

pytesseract

pdf2image

Pillow

tqdm

ocrmypdf

langchain==0.1.16

sentence-transformers

chromadb

EOF

  

echo "ðŸ§  Writing app.py (Streamlit entry point)..."

cat > "$ROOT/app.py" <<EOF

import streamlit as st

from sentence_transformers import SentenceTransformer

import chromadb

  

st.set_page_config(page_title="RAG QA with PDF Knowledge", layout="wide")

st.title("ðŸ” RAG QA from Your PDFs")

  

query = st.text_input("Ask a question based on your documents:")

  

if query:

model = SentenceTransformer('all-MiniLM-L6-v2')

chroma_client = chromadb.Client()

collection = chroma_client.get_collection("rag_chunks")

query_vector = model.encode(query).tolist()

results = collection.query(query_embeddings=[query_vector], n_results=3)

contexts = results['documents'][0]

  

st.subheader("ðŸ“„ Retrieved Contexts")

for i, chunk in enumerate(contexts):

st.markdown(f"**Chunk {i+1}:**")

st.code(chunk, language="markdown")

  

context_str = "\\n---\\n".join(contexts)

full_prompt = f"""Context:

{context_str}

  

Question: {query}

Answer:"""

  

st.subheader("ðŸ§  Constructed Prompt")

st.code(full_prompt, language="markdown")

st.info("ðŸ“Œ Copy this prompt into your local LLM to get a response.")

else:

st.warning("Enter a question to begin.")

EOF

  

echo "ðŸ§ª Writing app/main.py..."

cat > "$ROOT/app/main.py" <<EOF

# PDF processing logic placeholder

print("Hello from main.py")

EOF

  

echo "âš™ï¸ Writing supervisord.conf..."

cat > "$ROOT/supervisord.conf" <<EOF

[supervisord]

nodaemon=true

logfile=/var/log/supervisord.log

logfile_maxbytes=10MB

logfile_backups=10

loglevel=info

pidfile=/tmp/supervisord.pid

  

[program:ollama]

command=ollama serve

autostart=true

autorestart=true

stderr_logfile=/var/log/ollama.err.log

stdout_logfile=/var/log/ollama.out.log

  

[program:streamlit]

command=streamlit run /app.py --server.port=8501 --server.address=0.0.0.0

autostart=true

autorestart=true

stderr_logfile=/var/log/streamlit.err.log

stdout_logfile=/var/log/streamlit.out.log

EOF

  

echo "ðŸ“– Creating README.md..."

cat > "$ROOT/README.md" <<EOF

# RAG QA with PDF Knowledge

  

This project sets up a RAG (Retrieval-Augmented Generation) app using Ollama and Streamlit.

  

## Structure

- **app.py**: Main Streamlit app

- **app/**: Logic and PDF artifacts

- **pdfs/**: Drop PDFs here

- **chunks/**, **ocr_output_pdfs/**, **extracted_text/**: Auto-generated

- **Dockerfile**: Build config

- **supervisord.conf**: Starts both Ollama + Streamlit

  

## Quick Start

  

\`\`\`bash

cd rag-ollama

docker build -t rag-ollama .

docker run -p 11434:11434 -p 8501:8501 -v "\$(pwd)/app/pdfs:/app/app/pdfs" rag-ollama

\`\`\`

  

Then go to http://localhost:8501.

  

## License

MIT

EOF

  

echo "âœ… All done! Project initialized in '$ROOT'"
```